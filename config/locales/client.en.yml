en:
  js:
    discourse_automation:
      scriptables:
        auto_researcher:
          title: "Auto Researcher"
          description: "A general LLM posting automation using the OpenAI Responses API."

          fields:
            creator:
              label: "Creator (required)"
              description: "User account to author the new topic."

            variables:
              label: "Variables (optional)"
              description: "Key/value variables available to your prompt as {{variable}}. Add as many as you need."

            prompt:
              label: "Prompt (required)"
              description: "Full user prompt sent to the model. Variables like {{range}} are substituted."

            system_prompt:
              label: "System prompt (optional)"
              description: "Optional system/developer instruction sent before the user prompt."

            model:
              label: "Model (required)"
              description: "OpenAI model name for the Responses API (e.g., gpt-4o, gpt-4o-mini, o3, gpt-4.1)."

            poll_timing:
              label: "Poll timing (optional)"
              description: "Seconds between status checks if the API returns an in-progress response. Default 2s."

            send_pm_with_full_response:
              label: "Send PM with full response (optional)"
              description: "Send the raw JSON API response to this user as a private message."

            category:
              label: "Category to post in (required)"
              description: "Category where the new topic will be created."

            stop:
              label: "stop (optional)"
              description: "Stop sequences. Add one or more entries."

            temperature:
              label: "temperature (optional)"
              description: "Sampling temperature (0–2). If unset, model default applies."

            top_p:
              label: "top_p (optional)"
              description: "Nucleus sampling probability (0–1). If unset, model default applies."

            presence_penalty:
              label: "presence_penalty (optional)"
              description: "Penalize new tokens based on whether they appear in the text so far (–2..2)."

            frequency_penalty:
              label: "frequency_penalty (optional)"
              description: "Penalize new tokens based on their frequency so far (–2..2)."

            seed:
              label: "seed (optional)"
              description: "If supported, fixed seed to improve reproducibility."

            include_sources:
              label: "Include sources in output"
              description: "Hints the model to include citations when using web search."

            enable_web_search:
              label: "Enable web search tool"
              description: "Adds OpenAI’s built-in web search tool to the request."

            web_search_depth:
              label: "Web search depth"
              description: "Context size used by the web search tool."
              choices:
                low: "low"
                medium: "medium"
                high: "high"

            reasoning_effort:
              label: "reasoning.effort (optional)"
              description: "For reasoning models (o-series / GPT-5): low/medium/high."
              choices:
                low: "low"
                medium: "medium"
                high: "high"

            responses_api_overrides:
              label: "Advanced: Overrides JSON (optional)"
              description: >
                Raw JSON to merge into the final request body. Use this for any
                Responses API parameter that isn’t explicitly exposed here
                (e.g., tool_choice, response_format, metadata, max_output_tokens,
                logprobs, parallel_tool_calls, etc.). Be sure it matches model capabilities.

            title_prompt_hint:
              label: "—"
              description: "The script will call the model again to generate a concise topic title from the body."

